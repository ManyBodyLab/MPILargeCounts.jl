var documenterSearchIndex = {"docs":
[{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/#MPILargeCounts.MPILargeCounts","page":"Reference","title":"MPILargeCounts.MPILargeCounts","text":"Chunked communication based on MPI.jl with arbitrary-size data.\n\n\n\n\n\n","category":"module"},{"location":"reference/#MPILargeCounts.Irecv!-Tuple{Any, MPI.Comm}","page":"Reference","title":"MPILargeCounts.Irecv!","text":"req = Irecv!(recvbuf, comm::Comm;\n        source::Integer=MPI.ANY_SOURCE, tag::Integer=MPI.ANY_TAG)\n\nStarts a nonblocking receive into the buffer recvbuf from MPI rank source of communicator comm using the message tag tag.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MPILargeCounts.Isend-Tuple{Any, MPI.Comm}","page":"Reference","title":"MPILargeCounts.Isend","text":"Isend(data, comm::Comm; dest::Integer, tag::Integer=0)\n\nStarts a nonblocking send of data to MPI rank dest of communicator comm using with the message tag tag.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MPILargeCounts.Recv!","page":"Reference","title":"MPILargeCounts.Recv!","text":"data = Recv!(recvbuf, comm::Comm;\n        source::Integer=MPI.ANY_SOURCE, tag::Integer=MPI.ANY_TAG)\n\nCompletes a blocking receive into the buffer recvbuf from MPI rank source of communicator comm using with the message tag tag.\n\n\n\n\n\n","category":"function"},{"location":"reference/#MPILargeCounts.Send-Tuple{Any, MPI.Comm}","page":"Reference","title":"MPILargeCounts.Send","text":"Send(buf, comm::Comm; dest::Integer, tag::Integer=0)\n\nPerform a blocking send from the buffer buf to MPI rank dest of communicator comm using the message tag tag.\n\nSend(obj, comm::Comm; dest::Integer, tag::Integer=0)\n\nComplete a blocking send of an isbits object obj to MPI rank dest of communicator comm using with the message tag tag.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MPILargeCounts.allreduce-Tuple{Any, Any, MPI.Comm}","page":"Reference","title":"MPILargeCounts.allreduce","text":"allreduce(obj, op, comm::Comm; root::Integer=0)\nallreduce!(obj, op, comm::Comm; root::Integer=0)\n\nPerforms elementwise reduction using the operator op on the object obj across all ranks in comm, returning the reduced object on all ranks.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MPILargeCounts.bcast-Tuple{Any, MPI.Comm}","page":"Reference","title":"MPILargeCounts.bcast","text":"bcast(obj, comm::Comm; root::Integer=0)\n\nBroadcast the object obj from rank root to all processes on comm. This is able to handle arbitrary data.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MPILargeCounts.isend-Tuple{Any, MPI.Comm}","page":"Reference","title":"MPILargeCounts.isend","text":"isend(obj, comm::Comm; dest::Integer, tag::Integer=0)\n\nStarts a nonblocking send of using a serialized version of obj to MPI rank dest of communicator comm using with the message tag tag.\n\nReturns the communication Request for the nonblocking send.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MPILargeCounts.mpi_execute_on_root-Union{Tuple{A}, Tuple{A, Vararg{Any}}} where A","page":"Reference","title":"MPILargeCounts.mpi_execute_on_root","text":"mpi_execute_on_root(F::A, args...; kwargs...)\n\nExecute the function F with arguments args... and keyword arguments kwargs... on the root of the communicator comm (default: MPI.COMM_WORLD). If blocking is set to true (default: false), a barrier is performed before and after the execution to synchronize all ranks. Returns the result of F on the root, and nothing on all other ranks.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MPILargeCounts.mpi_execute_on_root_and_bcast-Union{Tuple{A}, Tuple{A, Vararg{Any}}} where A","page":"Reference","title":"MPILargeCounts.mpi_execute_on_root_and_bcast","text":"mpi_execute_on_root_and_bcast(F::A, args...; kwargs...)\n\nExecute the function F with arguments args... and keyword arguments kwargs... on the root of the communicator comm (default: MPI.COMM_WORLD), and broadcast the result to all ranks. Broadcasts the result of F on all ranks.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MPILargeCounts.recv","page":"Reference","title":"MPILargeCounts.recv","text":"obj = recv(comm::Comm;\n        source::Integer=MPI.ANY_SOURCE, tag::Integer=MPI.ANY_TAG)\n\nCompletes a blocking receive of a serialized object from MPI rank source of communicator comm using with the message tag tag.\n\n\n\n\n\n","category":"function"},{"location":"reference/#MPILargeCounts.reduce-Tuple{Any, Any, MPI.Comm}","page":"Reference","title":"MPILargeCounts.reduce","text":"reduce(obj, op, comm::Comm; root::Integer=0)\nreduce!(obj, op, comm::Comm; root::Integer=0)\n\nPerforms elementwise reduction using the operator op on the object obj on the  root rank root of comm, returning the reduced object on rank root.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MPILargeCounts.send-Tuple{Any, MPI.Comm}","page":"Reference","title":"MPILargeCounts.send","text":"send(obj, comm::Comm; dest::Integer, tag::Integer=0)\n\nComplete a blocking send using a serialized version of obj to MPI rank dest of communicator comm using with the message tag tag.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MPILargeCounts.split_buffer-Tuple{AbstractVector}","page":"Reference","title":"MPILargeCounts.split_buffer","text":"split_buffer(vec)\n\nSplit an AbstractVector into a Vector{MPI.Buffer} views without copying.\n\n\n\n\n\n","category":"method"},{"location":"index/#Index","page":"Index","title":"Index","text":"","category":"section"},{"location":"#MPILargeCounts.jl","page":"Home","title":"MPILargeCounts.jl","text":"MPILargeCounts.jl is a Julia package that provides support for arbitrarily large MPI operations instead of the native typemax(Cint) limit. This is achieved by chunking the messages into smaller pieces and performing the collective operations on these chunks sequentially. The corresponding functions have the same name as in MPI.jl, but are not exported to avoid name clashes. To use the functions instead of the MPI.jlones, you have to prefix them with MPILargeCounts..","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"The package is not yet registered in the Julia general registry. It can be installed trough the package manager with the following command:\n\npkg> add git@github.com:ManyBodyLab/MPILargeCounts.jl.git","category":"section"},{"location":"#Code-Samples","page":"Home","title":"Code Samples","text":"When running with e.g. 2 MPI ranks:\n\njulia> using MPI, MPILargeCounts\njulia> MPI.Init()\njulia> A = collect(1:Int(typemax(Cint))+10);\njulia> B = MPI.bcast(A, comm; root=0); # errors\njulia> B = MPILargeCounts.bcast(A, comm; root=0);\njulia> B == A\ntrue\ntrue","category":"section"},{"location":"#License","page":"Home","title":"License","text":"MPILargeCounts.jl is licensed under the MIT License). By using or interacting with this software in any way, you agree to the license of this software.\n\n\n\nThis page was generated using Literate.jl.","category":"section"}]
}
